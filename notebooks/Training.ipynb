{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96beffbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 1\n",
      "\n",
      "Accuracies: [82.98]\n",
      "\n",
      "Entropies: [685863]\n",
      "\n",
      "Max Accuracy: 82.98\n",
      "Min entropy: 685863\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 35.77 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Accuracies: [82.98, 89.54]\n",
      "\n",
      "Entropies: [685863, 685861]\n",
      "\n",
      "Max Accuracy: 89.54\n",
      "Min entropy: 685861\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 35.62 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65]\n",
      "\n",
      "Entropies: [685863, 685861, 685845]\n",
      "\n",
      "Max Accuracy: 92.65\n",
      "Min entropy: 685845\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 35.39 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825]\n",
      "\n",
      "Max Accuracy: 93.71\n",
      "Min entropy: 685825\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 34.25 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71, 94.89]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825, 685554]\n",
      "\n",
      "Max Accuracy: 94.89\n",
      "Min entropy: 685554\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 36.02 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 6\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71, 94.89, 95.65]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825, 685554, 685437]\n",
      "\n",
      "Max Accuracy: 95.65\n",
      "Min entropy: 685437\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 34.35 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 7\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71, 94.89, 95.65, 96.22]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825, 685554, 685437, 685502]\n",
      "\n",
      "Max Accuracy: 96.22\n",
      "Min entropy: 685437\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 35.00 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 8\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71, 94.89, 95.65, 96.22, 96.58]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825, 685554, 685437, 685502, 685366]\n",
      "\n",
      "Max Accuracy: 96.58\n",
      "Min entropy: 685366\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 34.06 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 9\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71, 94.89, 95.65, 96.22, 96.58, 96.73]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825, 685554, 685437, 685502, 685366, 685493]\n",
      "\n",
      "Max Accuracy: 96.73\n",
      "Min entropy: 685366\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 35.28 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 10\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71, 94.89, 95.65, 96.22, 96.58, 96.73, 97.0]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825, 685554, 685437, 685502, 685366, 685493, 685514]\n",
      "\n",
      "Max Accuracy: 97.0\n",
      "Min entropy: 685366\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 34.29 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 11\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71, 94.89, 95.65, 96.22, 96.58, 96.73, 97.0, 97.02]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825, 685554, 685437, 685502, 685366, 685493, 685514, 685592]\n",
      "\n",
      "Max Accuracy: 97.02\n",
      "Min entropy: 685366\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 34.08 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 12\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71, 94.89, 95.65, 96.22, 96.58, 96.73, 97.0, 97.02, 97.5]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825, 685554, 685437, 685502, 685366, 685493, 685514, 685592, 685602]\n",
      "\n",
      "Max Accuracy: 97.5\n",
      "Min entropy: 685366\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 34.32 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 13\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71, 94.89, 95.65, 96.22, 96.58, 96.73, 97.0, 97.02, 97.5, 97.71]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825, 685554, 685437, 685502, 685366, 685493, 685514, 685592, 685602, 685396]\n",
      "\n",
      "Max Accuracy: 97.71\n",
      "Min entropy: 685366\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 36.39 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 14\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71, 94.89, 95.65, 96.22, 96.58, 96.73, 97.0, 97.02, 97.5, 97.71, 97.61]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825, 685554, 685437, 685502, 685366, 685493, 685514, 685592, 685602, 685396, 685477]\n",
      "\n",
      "Max Accuracy: 97.71\n",
      "Min entropy: 685366\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 40.19 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 15\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71, 94.89, 95.65, 96.22, 96.58, 96.73, 97.0, 97.02, 97.5, 97.71, 97.61, 97.96]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825, 685554, 685437, 685502, 685366, 685493, 685514, 685592, 685602, 685396, 685477, 685354]\n",
      "\n",
      "Max Accuracy: 97.96\n",
      "Min entropy: 685354\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 46.07 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 16\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71, 94.89, 95.65, 96.22, 96.58, 96.73, 97.0, 97.02, 97.5, 97.71, 97.61, 97.96, 97.94]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825, 685554, 685437, 685502, 685366, 685493, 685514, 685592, 685602, 685396, 685477, 685354, 685623]\n",
      "\n",
      "Max Accuracy: 97.96\n",
      "Min entropy: 685354\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 42.64 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 17\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71, 94.89, 95.65, 96.22, 96.58, 96.73, 97.0, 97.02, 97.5, 97.71, 97.61, 97.96, 97.94, 98.03]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825, 685554, 685437, 685502, 685366, 685493, 685514, 685592, 685602, 685396, 685477, 685354, 685623, 685574]\n",
      "\n",
      "Max Accuracy: 98.03\n",
      "Min entropy: 685354\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 36.75 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 18\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71, 94.89, 95.65, 96.22, 96.58, 96.73, 97.0, 97.02, 97.5, 97.71, 97.61, 97.96, 97.94, 98.03, 98.29]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825, 685554, 685437, 685502, 685366, 685493, 685514, 685592, 685602, 685396, 685477, 685354, 685623, 685574, 685585]\n",
      "\n",
      "Max Accuracy: 98.29\n",
      "Min entropy: 685354\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 40.28 seconds\n",
      "\n",
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 19\n",
      "\n",
      "Accuracies: [82.98, 89.54, 92.65, 93.71, 94.89, 95.65, 96.22, 96.58, 96.73, 97.0, 97.02, 97.5, 97.71, 97.61, 97.96, 97.94, 98.03, 98.29, 98.29]\n",
      "\n",
      "Entropies: [685863, 685861, 685845, 685825, 685554, 685437, 685502, 685366, 685493, 685514, 685592, 685602, 685396, 685477, 685354, 685623, 685574, 685585, 685486]\n",
      "\n",
      "Max Accuracy: 98.29\n",
      "Min entropy: 685354\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 35.43 seconds\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 497\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m    496\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 497\u001b[0m accuracy, entropy, target_acc, target_entr \u001b[38;5;241m=\u001b[39m train_and_evaluate(C\u001b[38;5;241m=\u001b[39mC,              \n\u001b[1;32m    498\u001b[0m                                                             lr\u001b[38;5;241m=\u001b[39mlr,           \n\u001b[1;32m    499\u001b[0m                                                             lambda_reg\u001b[38;5;241m=\u001b[39mlambda_reg,    \n\u001b[1;32m    500\u001b[0m                                                             alpha\u001b[38;5;241m=\u001b[39malpha,          \n\u001b[1;32m    501\u001b[0m                                                             subgradient_step\u001b[38;5;241m=\u001b[39msubgradient_step, \n\u001b[1;32m    502\u001b[0m                                                             w0\u001b[38;5;241m=\u001b[39mw0,             \n\u001b[1;32m    503\u001b[0m                                                             r\u001b[38;5;241m=\u001b[39mr,              \n\u001b[1;32m    504\u001b[0m                                                             target_acc\u001b[38;5;241m=\u001b[39mtarget_acc,      \n\u001b[1;32m    505\u001b[0m                                                             target_entr\u001b[38;5;241m=\u001b[39mtarget_entr, \n\u001b[1;32m    506\u001b[0m                                                             min_xi\u001b[38;5;241m=\u001b[39mmin_xi,              \n\u001b[1;32m    507\u001b[0m                                                             max_xi\u001b[38;5;241m=\u001b[39mmax_xi,             \n\u001b[1;32m    508\u001b[0m                                                             n_epochs\u001b[38;5;241m=\u001b[39mn_epochs,        \n\u001b[1;32m    509\u001b[0m                                                             device\u001b[38;5;241m=\u001b[39mdevice,      \n\u001b[1;32m    510\u001b[0m                                                             train_optimizer\u001b[38;5;241m=\u001b[39mtrain_optimizer,     \n\u001b[1;32m    511\u001b[0m                                                             entropy_optimizer\u001b[38;5;241m=\u001b[39mentropy_optimizer,   \n\u001b[1;32m    512\u001b[0m                                                             trainloader\u001b[38;5;241m=\u001b[39mtrainloader, \n\u001b[1;32m    513\u001b[0m                                                             testloader\u001b[38;5;241m=\u001b[39mtestloader     \n\u001b[1;32m    514\u001b[0m                                                         )\n\u001b[1;32m    516\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime spent to train the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 361\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(C, lr, lambda_reg, alpha, subgradient_step, w0, r, target_acc, target_entr, min_xi, max_xi, n_epochs, device, train_optimizer, entropy_optimizer, trainloader, testloader)\u001b[0m\n\u001b[1;32m    359\u001b[0m l \u001b[38;5;241m=\u001b[39m l \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1.5\u001b[39m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(entropy_optimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 361\u001b[0m     xi, beta_tensor, x_star, phi \u001b[38;5;241m=\u001b[39m FISTA(xi, v, w_quantized, C, subgradient_step, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m) \n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m(entropy_optimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPM\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    363\u001b[0m     xi, beta_tensor, x_star, phi \u001b[38;5;241m=\u001b[39m ProximalBM(xi, v, w_quantized, C, zeta, subgradient_step, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)      \n",
      "Cell \u001b[0;32mIn[4], line 195\u001b[0m, in \u001b[0;36mFISTA\u001b[0;34m(xi, v, w, C, subgradient_step, max_iterations)\u001b[0m\n\u001b[1;32m    191\u001b[0m t_prev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_iterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Solve the simil-knapsack problem for the current xi\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m     x_i_star, lambda_plus, phi_plus \u001b[38;5;241m=\u001b[39m knapsack_specialized(xi, v, w, C)\n\u001b[1;32m    196\u001b[0m     sum_x_star \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(x_i_star, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# Compute the optimal c values c_star\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 132\u001b[0m, in \u001b[0;36mknapsack_specialized\u001b[0;34m(xi, v, w, C)\u001b[0m\n\u001b[1;32m    129\u001b[0m idx_left \u001b[38;5;241m=\u001b[39m indices_breakpoints[search_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Correct the indices for extreme cases\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m idx_right \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(mask_right, indices_breakpoints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], idx_right)\n\u001b[1;32m    133\u001b[0m idx_left \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(mask_right, indices_breakpoints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], idx_left)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Correct the indices for the case when w < v[0]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import product\n",
    "import math\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "from torch.linalg import norm\n",
    "\n",
    "# Define the LeNet-5 architecture\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # First convolutional layer: input 1 channel, output 6 channels, kernel size 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # Average pooling layer with kernel size 2x2 and stride 2\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        # Second convolutional layer: input 6 channels, output 16 channels, kernel size 5x5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)  # Flattened input size 16*4*4, output 120\n",
    "        self.fc2 = nn.Linear(120, 84)  # Output 84 neurons\n",
    "        self.fc3 = nn.Linear(84, 10)  # Output 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolution, then activation function (tanh), then pooling\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # Apply second convolution, then activation function (tanh), then pooling\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        # Pass through fully connected layers with activation (tanh)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        # Output layer without activation (raw scores for classification)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def compute_entropy(string):\n",
    "    \"\"\"\n",
    "    Function to compute the entropy of a given string.\n",
    "    \"\"\"\n",
    "    # Count the frequency of each character in the string\n",
    "    frequencies = Counter(string)\n",
    "    # Calculate the total length of the string\n",
    "    total_length = len(string)\n",
    "    \n",
    "    # Compute entropy\n",
    "    entropy = 0\n",
    "    for freq in frequencies.values():\n",
    "        probability = freq / total_length  # Compute probability of each character\n",
    "        entropy -= probability * math.log2(probability)  # Apply entropy formula\n",
    "    \n",
    "    return entropy * total_length  # Return the entropy weighted by string length\n",
    "            \n",
    "def test_accuracy(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Function to calculate the accuracy of a model on a given dataloader.\n",
    "    \"\"\"\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to the appropriate device\n",
    "            outputs = model(images)  # Get model predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the class with the highest probability\n",
    "            total += labels.size(0)  # Update total number of samples\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "    \n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "    return accuracy\n",
    "\n",
    "def knapsack_specialized(xi, v, w, C):\n",
    "    \"\"\"\n",
    "    Solves a specialized knapsack problem using a specialized method in a vectorized way\n",
    "\n",
    "    Args:\n",
    "        xi (torch.Tensor): xi variables.\n",
    "        v (torch.Tensor): Quantization vector.\n",
    "        w (torch.Tensor): Weight vector.\n",
    "        C (int): Number of buckets of quantization.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Optimal allocation (x_opt), optimal multipliers (lambda_opt), and objective values.\n",
    "    \"\"\"\n",
    "    \n",
    "    b_list = []\n",
    "    b = 0\n",
    "\n",
    "    # Compute breakpoint vector x_plus\n",
    "    while True:\n",
    "        delta_xi = (xi[b + 1:] - xi[b])\n",
    "        delta_v = (v[b + 1:] - v[b])\n",
    "        b = torch.argmin(delta_xi / delta_v) + 1 + b_list[-1] if b_list else 0\n",
    "\n",
    "        if b != C - 1:\n",
    "            b_list.append(int(b))\n",
    "\n",
    "        if b + 1 > C - 1:\n",
    "            break\n",
    "    b_list.append(C - 1)\n",
    "    x_plus = torch.zeros(C, dtype=torch.int32)\n",
    "    b_tensor = torch.tensor(b_list, dtype=torch.int32)\n",
    "    x_plus[b_tensor] = 1\n",
    "\n",
    "    # Determine optimal allocation based on w\n",
    "    w_idx = torch.searchsorted(v, w) \n",
    "    indices_breakpoints = torch.nonzero(x_plus == 1).squeeze()\n",
    "\n",
    "    # Creation of masks for extreme cases\n",
    "    mask_right = w > v[-1]\n",
    "    mask_left = w < v[0]\n",
    "\n",
    "    # Find indices using searchsorted\n",
    "    search_idx = torch.searchsorted(indices_breakpoints, w_idx)\n",
    "\n",
    "    # Ensure that the indices are valid\n",
    "    search_idx = torch.clamp(search_idx, 1, len(indices_breakpoints) - 1)\n",
    "\n",
    "    # Initialize idx_right and idx_left with the result of the search\n",
    "    idx_right = indices_breakpoints[search_idx]\n",
    "    idx_left = indices_breakpoints[search_idx - 1]\n",
    "\n",
    "    # Correct the indices for extreme cases\n",
    "    idx_right = torch.where(mask_right, indices_breakpoints[-1], idx_right)\n",
    "    idx_left = torch.where(mask_right, indices_breakpoints[-1], idx_left)\n",
    "\n",
    "    # Correct the indices for the case when w < v[0]\n",
    "    idx_right = torch.where(mask_left, indices_breakpoints[0], idx_right)\n",
    "    idx_left = torch.where(mask_left, indices_breakpoints[0], idx_left)\n",
    "\n",
    "    # Compute convex combination for optimal solution\n",
    "    x1, x2 = torch.zeros(2, len(w), C, dtype=torch.float32)\n",
    "\n",
    "    x1[torch.arange(len(w)), idx_left] = 1\n",
    "    x2[torch.arange(len(w)), idx_right] = 1\n",
    "\n",
    "    numerator = w - torch.matmul(x2, v)\n",
    "    denominator = torch.matmul((x1 - x2), v)\n",
    "    theta = numerator / denominator\n",
    "\n",
    "    mask_equal = (x1 == x2)\n",
    "    theta_expanded = theta.unsqueeze(1)\n",
    "    x_opt = torch.where(mask_equal, x1, x1 * theta_expanded + x2 * (1 - theta_expanded))\n",
    "\n",
    "    # Compute optimal multipliers\n",
    "    denominator = (v[idx_right] - v[idx_left])\n",
    "    denominator_zero_mask = denominator == 0\n",
    "\n",
    "    lambda_opt_nonzero = (xi[idx_right] - xi[idx_left]) / denominator\n",
    "    lambda_opt_zero_full = xi / v\n",
    "    lambda_opt_zero_full[0] = 0\n",
    "    lambda_opt_zero = lambda_opt_zero_full[idx_left]\n",
    "\n",
    "    lambda_opt = torch.where(denominator_zero_mask, lambda_opt_zero, lambda_opt_nonzero)\n",
    "\n",
    "    # Compute objective function values\n",
    "    objective_values = torch.matmul(x_opt, xi)\n",
    "\n",
    "    return x_opt, lambda_opt, objective_values\n",
    "\n",
    "def FISTA(xi, v, w, C, subgradient_step, max_iterations):\n",
    "    \"\"\"\n",
    "    Implements the Fast Iterative Shrinking-Thresholding Algorithm (FISTA) \n",
    "    for optimizing a constrained objective function.\n",
    "\n",
    "    Args:\n",
    "        xi (torch.Tensor): Initial parameter vector.\n",
    "        v (torch.Tensor): Constraint-related vector.\n",
    "        w (torch.Tensor): Weight vector.\n",
    "        C (float): Constraint parameter.\n",
    "        subgradient_step (float): Step size for subgradient descent.\n",
    "        max_iterations (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated xi, lambda_plus (Lagrange multiplier), \n",
    "               x_i_star (optimal allocation), and phi (objective function value).\n",
    "    \"\"\"\n",
    "    \n",
    "    upper_c = w.size(0)  # Define an upper bound for constraints\n",
    "    \n",
    "    # Initialize previous values for FISTA acceleration\n",
    "    xi_prev = xi.clone()\n",
    "    t_prev = torch.tensor(1.0)\n",
    "\n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        # Solve the simil-knapsack problem for the current xi\n",
    "        x_i_star, lambda_plus, phi_plus = knapsack_specialized(xi, v, w, C)\n",
    "        sum_x_star = torch.sum(x_i_star, dim=0)\n",
    "\n",
    "        # Compute the optimal c values c_star\n",
    "        c_star = torch.exp(torch.log(torch.tensor(2)) * xi - 1)\n",
    "        c_star = torch.clamp(c_star, min=0, max=upper_c)\n",
    "\n",
    "        # Compute the super-gradient\n",
    "        g = -(c_star - sum_x_star)\n",
    "        \n",
    "        # Compute the 3 pieces of the objective function value phi and put them together\n",
    "        phi1 = torch.sum(c_star * torch.log(c_star) / torch.log(torch.tensor(2)))\n",
    "        phi2 = -torch.sum(xi * c_star)\n",
    "        phi3 = torch.sum(xi * sum_x_star)\n",
    "        phi = phi1 + phi2 + phi3\n",
    "\n",
    "        # FISTA acceleration step\n",
    "        t_current = (1 + torch.sqrt(1 + 4 * t_prev**2)) / 2\n",
    "        y = xi + ((t_prev - 1) / t_current) * (xi - xi_prev)\n",
    "\n",
    "        # Gradient update step\n",
    "        xi_next = y + (1 / subgradient_step) * g \n",
    "\n",
    "        # Update variables for next iteration\n",
    "        xi_prev = xi.clone()\n",
    "        xi = xi_next.clone()\n",
    "        t_prev = t_current\n",
    "\n",
    "        # Ensure xi remains sorted\n",
    "        xi = torch.sort(xi)[0]\n",
    "\n",
    "    return xi, lambda_plus, x_i_star, phi\n",
    "\n",
    "def ProximalBM(xi, v, w, C, zeta, subgradient_step, max_iterations):\n",
    "    \"\"\"\n",
    "    Implements the Proximal Bundle Method (PBM) for solving constrained \n",
    "    optimization problems using bundle techniques.\n",
    "\n",
    "    Args:\n",
    "        xi (torch.Tensor): Initial parameter vector.\n",
    "        v (torch.Tensor): Constraint-related vector.\n",
    "        w (torch.Tensor): Weight vector.\n",
    "        C (float): Constraint parameter.\n",
    "        zeta (float): Regularization parameter for proximal term.\n",
    "        subgradient_step (float): Step size for subgradient descent.\n",
    "        max_iterations (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated xi, lambda_plus (Lagrange multiplier), \n",
    "               x_i_star (optimal allocation), and phi (objective function value).\n",
    "    \"\"\"\n",
    "    \n",
    "    upper_c = w.size(0)  # Define an upper bound for constraints\n",
    "\n",
    "    # Parameters for the bundle method\n",
    "    epsilon = 1e-5  # Convergence tolerance\n",
    "    bundle_size = 5  # Maximum bundle size\n",
    "    bundle = []  # Initialize the bundle (list of points, phi values, and gradients)\n",
    "\n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        # Solve the knapsack problem for the current xi\n",
    "        x_i_star, lambda_plus, phi_plus = knapsack_specialized(xi, v, w, C)\n",
    "        sum_x_star = torch.sum(x_i_star, dim=0)\n",
    "\n",
    "        # Compute the optimal c values c_star\n",
    "        c_star = torch.exp(torch.log(torch.tensor(2)) * xi - 1)\n",
    "        c_star = torch.clamp(c_star, min=0, max=upper_c)\n",
    "\n",
    "        # Compute the super-gradient\n",
    "        g = -(c_star - sum_x_star)\n",
    "\n",
    "        # Compute the objective function value phi\n",
    "        phi1 = torch.sum(c_star * torch.log(c_star) / torch.log(torch.tensor(2)))\n",
    "        phi2 = -torch.sum(xi * c_star)\n",
    "        phi3 = torch.sum(xi * sum_x_star)\n",
    "        phi = phi1 + phi2 + phi3\n",
    "\n",
    "        # Add the current point to the bundle\n",
    "        bundle.append((xi.clone(), phi, g.clone()))\n",
    "        if len(bundle) > bundle_size:\n",
    "            bundle.pop(0)  # Remove the oldest point if the bundle exceeds max size\n",
    "\n",
    "        # Solve the quadratic regularization subproblem\n",
    "        bundle_points = torch.stack([item[0] for item in bundle])  # Bundle points\n",
    "        bundle_phis = torch.tensor([item[1] for item in bundle])  # Phi values\n",
    "        bundle_gradients = torch.stack([item[2] for item in bundle])  # Gradient values\n",
    "\n",
    "        # Construct the quadratic approximation model\n",
    "        diff = xi - bundle_points\n",
    "        model_phi = bundle_phis + torch.sum(bundle_gradients * diff, dim=1)\n",
    "        proximal_term = (zeta / 2) * norm(diff, dim=1)**2\n",
    "        subproblem_objective = model_phi + proximal_term\n",
    "\n",
    "        # Determine the next xi by minimizing the subproblem objective\n",
    "        best_idx = torch.argmin(subproblem_objective)\n",
    "        xi_next = bundle_points[best_idx] + (1 / zeta) * bundle_gradients[best_idx]\n",
    "\n",
    "        # Clip xi to enforce constraints\n",
    "        xi_next = torch.clamp(xi_next, min=0.01, max=upper_c)\n",
    "\n",
    "        # Check for convergence\n",
    "        if norm(xi_next - xi) < epsilon:\n",
    "            break\n",
    "\n",
    "        # Update xi for the next iteration\n",
    "        xi = xi_next.clone()\n",
    "        \n",
    "    return xi, lambda_plus, x_i_star, phi\n",
    "\n",
    "def initialize_weights(model, min_w, max_w):\n",
    "    \"\"\"\n",
    "    Initializes the weights of a given model using a uniform distribution.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model whose weights need initialization.\n",
    "        min_w (float): Minimum value for weight initialization.\n",
    "        max_w (float): Maximum value for weight initialization.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        torch.nn.init.uniform_(param, a=min_w, b=max_w)\n",
    "\n",
    "def train_and_evaluate(C, lr, lambda_reg, alpha, subgradient_step, w0, r, \n",
    "                       target_acc, target_entr, min_xi, max_xi, n_epochs, device, \n",
    "                       train_optimizer, entropy_optimizer, trainloader, testloader):\n",
    "    \n",
    "    model = LeNet5().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if(train_optimizer == 'A'):\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=lambda_reg * alpha)\n",
    "    elif(train_optimizer == 'S'):\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=lambda_reg * alpha)\n",
    "    \n",
    "    # Parameters initialization\n",
    "    min_w, max_w = w0 - r, w0 + r\n",
    "    v = torch.linspace(min_w, max_w - (max_w - min_w)/C, steps=C)\n",
    "    initialize_weights(model, min_w, max_w)    \n",
    "    w = torch.cat([param.data.view(-1) for param in model.parameters()])\n",
    "    upper_c, lower_c = w.size(0), 1e-2\n",
    "    xi = min_xi + (max_xi - min_xi) * torch.rand(C, device=device)    \n",
    "    xi = torch.sort(xi)[0]   \n",
    "    entropy, accuracy = 0, 0\n",
    "    accuracies, entropies, distinct_weights = [], [], []\n",
    "    zeta, l = 50000, 0.5\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            w = torch.cat([param.data.view(-1) for param in model.parameters()])\n",
    "            unique_weights = torch.unique(w).numel() \n",
    "            indices = torch.searchsorted(v, w, right=True) - 1\n",
    "            indices = torch.clamp(indices, min=0)\n",
    "            w_quantized = v[indices]\n",
    "\n",
    "            zeta *= 1 + l\n",
    "            l = l / 1.5\n",
    "            if(entropy_optimizer == 'F'):\n",
    "                xi, beta_tensor, x_star, phi = FISTA(xi, v, w_quantized, C, subgradient_step, max_iterations=15) \n",
    "            elif(entropy_optimizer == 'PM'):\n",
    "                xi, beta_tensor, x_star, phi = ProximalBM(xi, v, w_quantized, C, zeta, subgradient_step, max_iterations=15)      \n",
    "            \n",
    "            # Update of âˆ‡É¸\n",
    "            idx = 0\n",
    "            for param in model.parameters():\n",
    "                numel = param.numel()\n",
    "                if param.grad is not None:\n",
    "                    param_grad = param.grad.view(-1)\n",
    "                else:\n",
    "                    param_grad = torch.zeros_like(param.data.view(-1))\n",
    "                param_grad += (1 - alpha) * lambda_reg * beta_tensor[idx:idx + numel]\n",
    "                param.grad = param_grad.view(param.size())\n",
    "                idx += numel\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        w = torch.cat([param.data.view(-1) for param in model.parameters()])\n",
    "        \n",
    "        entropy = round(compute_entropy(w.tolist())) + 1\n",
    "        entropies.append(entropy)\n",
    "        accuracy = test_accuracy(model, testloader, device)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"C={C}, lr={lr}, lambda_reg={lambda_reg}, \"\n",
    "              f\"alpha={alpha}, subgradient_step={subgradient_step}, w0={w0}, r={r}, \"\n",
    "              f\"target_acc={target_acc}, target_entr={target_entr}, \"\n",
    "              f\"min_xi={min_xi}, max_xi={max_xi}, n_epochs={n_epochs}, train_optimizer={train_optimizer} \"\n",
    "              f\"entropy_optimizer={entropy_optimizer}\")\n",
    "        print(\"\\nEpoch:\", epoch+1)\n",
    "        print(\"\\nAccuracies:\", accuracies)\n",
    "        print(\"\\nEntropies:\", entropies)\n",
    "        print(\"\\nMax Accuracy:\", max(accuracies))\n",
    "        print(\"Min entropy:\", min(entropies))\n",
    "\n",
    "        # Saving a better model\n",
    "        if(accuracy >= target_acc and entropy <= target_entr):\n",
    "            print(\"ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥\\nðŸ’¥ATTENTION!ðŸ’¥\\nðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥ðŸ’¥\")\n",
    "            torch.save(model.state_dict(), f\"BestModelsBeforeQuantization/C{C}_r{round(r*1000)}.pth\")\n",
    "            target_acc = accuracy\n",
    "            target_entr = entropy\n",
    "        \n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        # Entropy exit conditions\n",
    "        if(epoch > 20 and entropy > 600000):\n",
    "            print(\"Entropy is not decreasing enough! (A)\")\n",
    "            return accuracy, entropy, target_acc, target_entr\n",
    "        if(epoch > 50):\n",
    "            if(entropies[-1] > 200000 and entropies[-2] > 200000 and entropies[-3] > 200000 and entropies[-4] > 200000):\n",
    "                print(\"Entropy is not decreasing enough! (B)\")\n",
    "                return accuracy, entropy, target_acc, target_entr           \n",
    "            \n",
    "        # Accuracy exit condition\n",
    "        if(epoch == 1 and accuracies[-1] < 70):\n",
    "            print(\"Accuracy is too low! (C)\")\n",
    "            return accuracy, entropy, target_acc, target_entr                    \n",
    "        if(epoch > 10):\n",
    "            if(accuracies[-1] < 90 and accuracies[-2] < 90 and accuracies[-3] < 90 and accuracies[-4] < 90):\n",
    "                print(\"Accuracy is too low! (D)\")\n",
    "                return accuracy, entropy, target_acc, target_entr     \n",
    "        \n",
    "        # ... ADD OTHER EXIT CONDITIONS ...      \n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Time taken for a epoch: {training_time:.2f} seconds\\n\")\n",
    "              \n",
    "    return accuracy, entropy, target_acc, target_entr\n",
    "\n",
    "# Select the computing device: use GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define a transformation: convert images to tensors\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "# Load the MNIST training dataset with the defined transformation\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# Create a DataLoader for the training set with batch size 64, shuffling enabled, and 4 worker threads\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\n",
    "# Load the MNIST test dataset with the same transformation\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "# Create a DataLoader for the test set with batch size 1000, shuffling disabled, and 4 worker threads\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False, num_workers=4)\n",
    "\n",
    "np.set_printoptions(precision=6)\n",
    "\n",
    "# Grid search \n",
    "param_grid = {\n",
    "    \"C\": [6, 12, 24, 48],  # Number of buckets of quantization\n",
    "    \"lr\": [0.0007], # Learning rate for the optimizer\n",
    "    \"lambda_reg\": [0.0015], # Regularization factor\n",
    "    \"alpha\": [0.533], # Percentage of standard regularization wrt entropic one \n",
    "    \"subgradient_step\": [1e5],  # Step size for subgradient\n",
    "    \"w0\": [-0.11], # Initial weight parameters\n",
    "    \"r\": [round(1.1 + i * 0.002, 3) for i in range(10)],\n",
    "    \"target_acc\": [98.99], # Target accuracy percentage\n",
    "    \"target_entr\": [0.99602e6], # Target entropy threshold \n",
    "    \"min_xi\": [0], # lower bound for xi initialization\n",
    "    \"max_xi\": [1],  # upper bound for xi initialization\n",
    "    \"n_epochs\": [100], # Number of training epochs\n",
    "    \"device\": [device], # Computing device (GPU or CPU)\n",
    "    \"train_optimizer\": ['A'],  # 'A' for Adam, and 'S' for SGD\n",
    "    \"entropy_optimizer\": ['F'], # 'F' for FISTA, 'PM' for proximal bundle\n",
    "    \"trainloader\": [trainloader],  # Training data loader\n",
    "    \"testloader\": [testloader] # Test data loader\n",
    "}\n",
    "\n",
    "combination = 0\n",
    "\n",
    "for (C, lr, lambda_reg, alpha, subgradient_step, w0, r, \n",
    "     target_acc, target_entr, min_xi, max_xi, n_epochs, \n",
    "     device, train_optimizer, entropy_optimizer, trainloader, \n",
    "     testloader) in product(param_grid[\"C\"],\n",
    "                            param_grid[\"lr\"],\n",
    "                            param_grid[\"lambda_reg\"],\n",
    "                            param_grid[\"alpha\"],\n",
    "                            param_grid[\"subgradient_step\"],\n",
    "                            param_grid[\"w0\"],\n",
    "                            param_grid[\"r\"],\n",
    "                            param_grid[\"target_acc\"],\n",
    "                            param_grid[\"target_entr\"],\n",
    "                            param_grid[\"min_xi\"],\n",
    "                            param_grid[\"max_xi\"],\n",
    "                            param_grid[\"n_epochs\"],\n",
    "                            param_grid[\"device\"],\n",
    "                            param_grid[\"train_optimizer\"],      \n",
    "                            param_grid[\"entropy_optimizer\"],   \n",
    "                            param_grid[\"trainloader\"], \n",
    "                            param_grid[\"testloader\"]\n",
    "                            ):\n",
    "    \n",
    "    # Counts combinations\n",
    "    combination += 1\n",
    "    \n",
    "    # Start training\n",
    "    start_time = time.time()\n",
    "    accuracy, entropy, target_acc, target_entr = train_and_evaluate(C=C,              \n",
    "                                                                lr=lr,           \n",
    "                                                                lambda_reg=lambda_reg,    \n",
    "                                                                alpha=alpha,          \n",
    "                                                                subgradient_step=subgradient_step, \n",
    "                                                                w0=w0,             \n",
    "                                                                r=r,              \n",
    "                                                                target_acc=target_acc,      \n",
    "                                                                target_entr=target_entr, \n",
    "                                                                min_xi=min_xi,              \n",
    "                                                                max_xi=max_xi,             \n",
    "                                                                n_epochs=n_epochs,        \n",
    "                                                                device=device,      \n",
    "                                                                train_optimizer=train_optimizer,     \n",
    "                                                                entropy_optimizer=entropy_optimizer,   \n",
    "                                                                trainloader=trainloader, \n",
    "                                                                testloader=testloader     \n",
    "                                                            )\n",
    "        \n",
    "    training_time = time.time() - start_time\n",
    "    print(f'Time spent to train the model: {training_time:.2f} seconds\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6787a54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1, 1.102, 1.104, 1.106, 1.108, 1.11, 1.112, 1.114, 1.116, 1.118]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[round(1.1 + i * 0.002, 3) for i in range(10)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
