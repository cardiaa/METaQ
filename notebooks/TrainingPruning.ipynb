{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96beffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=6, lr=0.0007, lambda_reg=0.0015, alpha=0.533, subgradient_step=100000.0, w0=-0.11, r=1.1, target_acc=98.99, target_entr=996020.0, min_xi=0, max_xi=1, n_epochs=100, train_optimizer=A entropy_optimizer=F\n",
      "\n",
      "Epoch: 1\n",
      "\n",
      "Accuracies: [56.05]\n",
      "\n",
      "Entropies: [685823]\n",
      "\n",
      "Max Accuracy: 56.05\n",
      "Min entropy: 685823\n",
      "------------------------------------------------------------\n",
      "Time taken for a epoch: 46.83 seconds\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 540\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m    539\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 540\u001b[0m accuracy, entropy, target_acc, target_entr \u001b[38;5;241m=\u001b[39m train_and_evaluate(C\u001b[38;5;241m=\u001b[39mC,              \n\u001b[1;32m    541\u001b[0m                                                             lr\u001b[38;5;241m=\u001b[39mlr,           \n\u001b[1;32m    542\u001b[0m                                                             lambda_reg\u001b[38;5;241m=\u001b[39mlambda_reg,    \n\u001b[1;32m    543\u001b[0m                                                             alpha\u001b[38;5;241m=\u001b[39malpha,          \n\u001b[1;32m    544\u001b[0m                                                             subgradient_step\u001b[38;5;241m=\u001b[39msubgradient_step, \n\u001b[1;32m    545\u001b[0m                                                             w0\u001b[38;5;241m=\u001b[39mw0,             \n\u001b[1;32m    546\u001b[0m                                                             r\u001b[38;5;241m=\u001b[39mr,              \n\u001b[1;32m    547\u001b[0m                                                             target_acc\u001b[38;5;241m=\u001b[39mtarget_acc,      \n\u001b[1;32m    548\u001b[0m                                                             target_entr\u001b[38;5;241m=\u001b[39mtarget_entr, \n\u001b[1;32m    549\u001b[0m                                                             min_xi\u001b[38;5;241m=\u001b[39mmin_xi,              \n\u001b[1;32m    550\u001b[0m                                                             max_xi\u001b[38;5;241m=\u001b[39mmax_xi,             \n\u001b[1;32m    551\u001b[0m                                                             n_epochs\u001b[38;5;241m=\u001b[39mn_epochs,        \n\u001b[1;32m    552\u001b[0m                                                             device\u001b[38;5;241m=\u001b[39mdevice,      \n\u001b[1;32m    553\u001b[0m                                                             train_optimizer\u001b[38;5;241m=\u001b[39mtrain_optimizer,     \n\u001b[1;32m    554\u001b[0m                                                             entropy_optimizer\u001b[38;5;241m=\u001b[39mentropy_optimizer,   \n\u001b[1;32m    555\u001b[0m                                                             trainloader\u001b[38;5;241m=\u001b[39mtrainloader, \n\u001b[1;32m    556\u001b[0m                                                             testloader\u001b[38;5;241m=\u001b[39mtestloader,\n\u001b[1;32m    557\u001b[0m                                                             delta\u001b[38;5;241m=\u001b[39mdelta\n\u001b[1;32m    558\u001b[0m                                                         )\n\u001b[1;32m    560\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime spent to train the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 401\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(C, lr, lambda_reg, alpha, subgradient_step, w0, r, target_acc, target_entr, min_xi, max_xi, n_epochs, device, train_optimizer, entropy_optimizer, trainloader, testloader, delta)\u001b[0m\n\u001b[1;32m    398\u001b[0m l \u001b[38;5;241m=\u001b[39m l \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1.5\u001b[39m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(entropy_optimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m#xi, beta_tensor, x_star, phi = FISTA(xi, v, w_quantized, C, subgradient_step, max_iterations=15) \u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     xi, beta_tensor, x_star, phi \u001b[38;5;241m=\u001b[39m FISTA(xi, v, w, C, subgradient_step, delta, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m) \n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m(entropy_optimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPM\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m#xi, beta_tensor, x_star, phi = ProximalBM(xi, v, w_quantized, C, zeta, subgradient_step, max_iterations=15) \u001b[39;00m\n\u001b[1;32m    404\u001b[0m     xi, beta_tensor, x_star, phi \u001b[38;5;241m=\u001b[39m ProximalBM(xi, v, w, C, zeta, subgradient_step, delta, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)      \n",
      "Cell \u001b[0;32mIn[12], line 234\u001b[0m, in \u001b[0;36mFISTA\u001b[0;34m(xi, v, w, C, subgradient_step, delta, max_iterations)\u001b[0m\n\u001b[1;32m    230\u001b[0m t_prev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_iterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# Solve the simil-knapsack problem for the current xi\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     x_i_star, lambda_plus, phi_plus \u001b[38;5;241m=\u001b[39m knapsack_specialized_pruning_parallel(xi, v, w, C, delta)\n\u001b[1;32m    235\u001b[0m     sum_x_star \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(x_i_star, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# Compute the optimal c values c_star\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 189\u001b[0m, in \u001b[0;36mknapsack_specialized_pruning_parallel\u001b[0;34m(xi, v, w, C, delta)\u001b[0m\n\u001b[1;32m    187\u001b[0m     i \u001b[38;5;241m=\u001b[39m indices[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    188\u001b[0m     lambda_opt[m1] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mxi[i] \u001b[38;5;241m/\u001b[39m v[i]\n\u001b[0;32m--> 189\u001b[0m     lambda_opt[m1] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(lambda_opt[m1], decimals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Caso 2 valori non nulli\u001b[39;00m\n\u001b[1;32m    192\u001b[0m m2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(nz_counts \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import product\n",
    "import math\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output\n",
    "from torch.linalg import norm\n",
    "\n",
    "# Define the LeNet-5 architecture\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # First convolutional layer: input 1 channel, output 6 channels, kernel size 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # Average pooling layer with kernel size 2x2 and stride 2\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        # Second convolutional layer: input 6 channels, output 16 channels, kernel size 5x5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)  # Flattened input size 16*4*4, output 120\n",
    "        self.fc2 = nn.Linear(120, 84)  # Output 84 neurons\n",
    "        self.fc3 = nn.Linear(84, 10)  # Output 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolution, then activation function (tanh), then pooling\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # Apply second convolution, then activation function (tanh), then pooling\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        # Pass through fully connected layers with activation (tanh)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        # Output layer without activation (raw scores for classification)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def compute_entropy(string):\n",
    "    \"\"\"\n",
    "    Function to compute the entropy of a given string.\n",
    "    \"\"\"\n",
    "    # Count the frequency of each character in the string\n",
    "    frequencies = Counter(string)\n",
    "    # Calculate the total length of the string\n",
    "    total_length = len(string)\n",
    "    \n",
    "    # Compute entropy\n",
    "    entropy = 0\n",
    "    for freq in frequencies.values():\n",
    "        probability = freq / total_length  # Compute probability of each character\n",
    "        entropy -= probability * math.log2(probability)  # Apply entropy formula\n",
    "    \n",
    "    return entropy * total_length  # Return the entropy weighted by string length\n",
    "            \n",
    "def test_accuracy(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Function to calculate the accuracy of a model on a given dataloader.\n",
    "    \"\"\"\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to the appropriate device\n",
    "            outputs = model(images)  # Get model predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the class with the highest probability\n",
    "            total += labels.size(0)  # Update total number of samples\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "    \n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "    return accuracy\n",
    "\n",
    "def knapsack_specialized_pruning_parallel(xi, v, w, C, delta):\n",
    "\n",
    "    xi = xi - delta\n",
    "\n",
    "    M = w.shape[0]\n",
    "\n",
    "    # Step 1: Calcolo x_plus (breakpoints)\n",
    "    b_list = []\n",
    "    b = 0\n",
    "    while True:\n",
    "        delta_xi = (xi[b + 1:] - xi[b])\n",
    "        delta_v = (v[b + 1:] - v[b])\n",
    "        b = torch.argmin(delta_xi / delta_v) + 1 + b_list[-1] if b_list else 0\n",
    "        if b != C - 1:\n",
    "            b_list.append(int(b))\n",
    "        if b + 1 > C - 1:\n",
    "            break\n",
    "    b_list.append(C - 1)\n",
    "    x_plus = torch.zeros(C, dtype=torch.int32)\n",
    "    x_plus[torch.tensor(b_list)] = 1\n",
    "\n",
    "    # Preallocazioni\n",
    "    x = torch.zeros(M, C)\n",
    "    lambda_opt = torch.zeros(M)\n",
    "\n",
    "    # Step 2: Classificazione dei problemi\n",
    "    v0 = v[0]\n",
    "    v_last = v[-1]\n",
    "    mask_small = w < v0\n",
    "    mask_large = w > v_last\n",
    "    mask_mid = (~mask_small) & (~mask_large)\n",
    "\n",
    "    # CASO: w > v[-1]\n",
    "    x[mask_large, -1] = 1\n",
    "\n",
    "    # CASO: w < v[0]\n",
    "    x[mask_small, 0] = 1\n",
    "\n",
    "    # CASO INTERMEDIO\n",
    "    if mask_mid.any():\n",
    "        M_mid = mask_mid.sum() #Numero di v[0]<w<v[-1]\n",
    "        w_mid = w[mask_mid]\n",
    "        ratio = xi / v\n",
    "        neg_indices = torch.where(ratio < 0)[0]\n",
    "        neg_sorted = neg_indices[torch.argsort(ratio[neg_indices], descending=True)]\n",
    "        pos_indices = torch.where(ratio >= 0)[0]\n",
    "        pos_sorted = pos_indices[torch.argsort(ratio[pos_indices])]\n",
    "        b_vector = torch.cat([neg_sorted, pos_sorted], dim=0)\n",
    "        ratio_b = w_mid[:, None] / v[b_vector]\n",
    "        x_plus_b = x_plus[b_vector].bool()\n",
    "        cond1 = (ratio_b >= 0) & x_plus_b\n",
    "        valid_i0 = cond1.float() * torch.arange(C)[None, :]\n",
    "        valid_i0[~cond1] = float('inf')\n",
    "        i0_pos = valid_i0.argmin(dim=1)\n",
    "        i0 = b_vector[i0_pos]\n",
    "        v_i0 = v[i0]\n",
    "        x_single = w_mid / v_i0\n",
    "        invalid_i0 = x_plus[i0] == 0\n",
    "        use_two = x_single > 1\n",
    "        i1 = torch.full_like(i0, fill_value=-1)\n",
    "        if use_two.any():\n",
    "            b_vector_exp = b_vector.unsqueeze(0).expand(M_mid, -1)\n",
    "            i0_exp = i0.unsqueeze(1).expand_as(b_vector_exp)\n",
    "            x_plus_mask = x_plus[b_vector_exp] == 1\n",
    "            greater_mask = b_vector_exp > i0_exp\n",
    "            valid_mask = x_plus_mask & greater_mask\n",
    "            masked_b_vector = torch.where(valid_mask, b_vector_exp, torch.full_like(b_vector_exp, C))\n",
    "            i1_candidate, _ = masked_b_vector.min(dim=1)\n",
    "            i1_candidate_use_two = i1_candidate[use_two]\n",
    "            valid_i1_use_two = i1_candidate_use_two < C\n",
    "            i0_use_two = i0[use_two]\n",
    "            i1[use_two] = torch.where(valid_i1_use_two, i1_candidate_use_two, i0_use_two)\n",
    "\n",
    "        # Costruzione x_mid\n",
    "        x_mid = torch.zeros(M_mid, C)\n",
    "\n",
    "        # Caso: uso un solo indice\n",
    "        mask_one = ~use_two\n",
    "        rows_one = torch.where(mask_one)[0]\n",
    "        cols_one = i0[mask_one]\n",
    "        x_mid[rows_one, cols_one] = torch.clamp(torch.round(w_mid[mask_one] / v[cols_one], decimals=5), 0.0, 1.0)\n",
    "\n",
    "        # Caso: combinazione convessa\n",
    "        mask_two = use_two & (i1 != i0)\n",
    "        rows_two = torch.where(mask_two)[0]\n",
    "        idx0 = i0[mask_two]\n",
    "        idx1 = i1[mask_two]\n",
    "        v0 = v[idx0]\n",
    "        v1 = v[idx1]\n",
    "        w_sel = w_mid[mask_two]\n",
    "        theta = (w_sel - v1) / (v0 - v1)\n",
    "        x_mid[rows_two, idx0] = torch.round(theta, decimals=5)\n",
    "        x_mid[rows_two, idx1] = torch.round(1 - theta, decimals=5)\n",
    "\n",
    "        x[mask_mid] = x_mid\n",
    "        \n",
    "    # === Calcolo vectorizzato dei moltiplicatori ===\n",
    "    eps = 1e-6\n",
    "    nz_mask = torch.abs(x) > eps\n",
    "    nz_counts = nz_mask.sum(dim=1)\n",
    "    lambda_opt = torch.zeros(x.shape[0])\n",
    "\n",
    "    # Caso 1 valore non nullo\n",
    "    m1 = torch.where(nz_counts == 1)[0]\n",
    "    if m1.numel() > 0:\n",
    "        submask = nz_mask[m1]\n",
    "        indices = submask.nonzero(as_tuple=False) \n",
    "        i = indices[:, 1]\n",
    "        lambda_opt[m1] = -xi[i] / v[i]\n",
    "        lambda_opt[m1] = torch.round(lambda_opt[m1], decimals=5)\n",
    "\n",
    "    # Caso 2 valori non nulli\n",
    "    m2 = torch.where(nz_counts == 2)[0]\n",
    "    if m2.numel() > 0:\n",
    "        indices = nz_mask[m2].nonzero().reshape(-1, 2)\n",
    "        grouped = indices.view(-1, 2, 2)\n",
    "        i = grouped[:, 0, 1]\n",
    "        j = grouped[:, 1, 1]\n",
    "        delta_xi = xi[j] - xi[i]\n",
    "        delta_idx = j - i\n",
    "        passo = v[1] - v[0]\n",
    "        lambda_opt[m2] = -delta_xi / (delta_idx * passo)\n",
    "        lambda_opt[m2] = torch.round(lambda_opt[m2], decimals=5)\n",
    "\n",
    "    objective_values = delta + x @ xi\n",
    "    \n",
    "    return x, lambda_opt, objective_values\n",
    "\n",
    "def FISTA(xi, v, w, C, subgradient_step, delta, max_iterations):\n",
    "    \"\"\"\n",
    "    Implements the Fast Iterative Shrinking-Thresholding Algorithm (FISTA) \n",
    "    for optimizing a constrained objective function.\n",
    "\n",
    "    Args:\n",
    "        xi (torch.Tensor): Initial parameter vector.\n",
    "        v (torch.Tensor): Constraint-related vector.\n",
    "        w (torch.Tensor): Weight vector.\n",
    "        C (float): Constraint parameter.\n",
    "        subgradient_step (float): Step size for subgradient descent.\n",
    "        max_iterations (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated xi, lambda_plus (Lagrange multiplier), \n",
    "               x_i_star (optimal allocation), and phi (objective function value).\n",
    "    \"\"\"\n",
    "    \n",
    "    upper_c = w.size(0)  # Define an upper bound for constraints\n",
    "    \n",
    "    # Initialize previous values for FISTA acceleration\n",
    "    xi_prev = xi.clone()\n",
    "    t_prev = torch.tensor(1.0)\n",
    "\n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        # Solve the simil-knapsack problem for the current xi\n",
    "        x_i_star, lambda_plus, phi_plus = knapsack_specialized_pruning_parallel(xi, v, w, C, delta)\n",
    "        sum_x_star = torch.sum(x_i_star, dim=0)\n",
    "\n",
    "        # Compute the optimal c values c_star\n",
    "        c_star = torch.exp(torch.log(torch.tensor(2)) * xi - 1)\n",
    "        c_star = torch.clamp(c_star, min=0, max=upper_c)\n",
    "\n",
    "        # Compute the super-gradient\n",
    "        g = -(c_star - sum_x_star)\n",
    "        \n",
    "        # Compute the 3 pieces of the objective function value phi and put them together\n",
    "        phi1 = torch.sum(c_star * torch.log(c_star) / torch.log(torch.tensor(2)))\n",
    "        phi2 = -torch.sum(xi * c_star)\n",
    "        phi3 = torch.sum(xi * sum_x_star)\n",
    "        phi = phi1 + phi2 + phi3\n",
    "\n",
    "        # FISTA acceleration step\n",
    "        t_current = (1 + torch.sqrt(1 + 4 * t_prev**2)) / 2\n",
    "        y = xi + ((t_prev - 1) / t_current) * (xi - xi_prev)\n",
    "\n",
    "        # Gradient update step\n",
    "        xi_next = y + (1 / subgradient_step) * g \n",
    "\n",
    "        # Update variables for next iteration\n",
    "        xi_prev = xi.clone()\n",
    "        xi = xi_next.clone()\n",
    "        t_prev = t_current\n",
    "\n",
    "        # Ensure xi remains sorted\n",
    "        xi = torch.sort(xi)[0]\n",
    "\n",
    "    return xi, lambda_plus, x_i_star, phi\n",
    "\n",
    "def ProximalBM(xi, v, w, C, zeta, subgradient_step, delta, max_iterations):\n",
    "    \"\"\"\n",
    "    Implements the Proximal Bundle Method (PBM) for solving constrained \n",
    "    optimization problems using bundle techniques.\n",
    "\n",
    "    Args:\n",
    "        xi (torch.Tensor): Initial parameter vector.\n",
    "        v (torch.Tensor): Constraint-related vector.\n",
    "        w (torch.Tensor): Weight vector.\n",
    "        C (float): Constraint parameter.\n",
    "        zeta (float): Regularization parameter for proximal term.\n",
    "        subgradient_step (float): Step size for subgradient descent.\n",
    "        max_iterations (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated xi, lambda_plus (Lagrange multiplier), \n",
    "               x_i_star (optimal allocation), and phi (objective function value).\n",
    "    \"\"\"\n",
    "    \n",
    "    upper_c = w.size(0)  # Define an upper bound for constraints\n",
    "\n",
    "    # Parameters for the bundle method\n",
    "    epsilon = 1e-5  # Convergence tolerance\n",
    "    bundle_size = 5  # Maximum bundle size\n",
    "    bundle = []  # Initialize the bundle (list of points, phi values, and gradients)\n",
    "\n",
    "    for iteration in range(1, max_iterations + 1):\n",
    "        # Solve the knapsack problem for the current xi\n",
    "        x_i_star, lambda_plus, phi_plus = knapsack_specialized_pruning_parallel(xi, v, w, C, delta)\n",
    "        sum_x_star = torch.sum(x_i_star, dim=0)\n",
    "\n",
    "        # Compute the optimal c values c_star\n",
    "        c_star = torch.exp(torch.log(torch.tensor(2)) * xi - 1)\n",
    "        c_star = torch.clamp(c_star, min=0, max=upper_c)\n",
    "\n",
    "        # Compute the super-gradient\n",
    "        g = -(c_star - sum_x_star)\n",
    "\n",
    "        # Compute the objective function value phi\n",
    "        phi1 = torch.sum(c_star * torch.log(c_star) / torch.log(torch.tensor(2)))\n",
    "        phi2 = -torch.sum(xi * c_star)\n",
    "        phi3 = torch.sum(xi * sum_x_star)\n",
    "        phi = phi1 + phi2 + phi3\n",
    "\n",
    "        # Add the current point to the bundle\n",
    "        bundle.append((xi.clone(), phi, g.clone()))\n",
    "        if len(bundle) > bundle_size:\n",
    "            bundle.pop(0)  # Remove the oldest point if the bundle exceeds max size\n",
    "\n",
    "        # Solve the quadratic regularization subproblem\n",
    "        bundle_points = torch.stack([item[0] for item in bundle])  # Bundle points\n",
    "        bundle_phis = torch.tensor([item[1] for item in bundle])  # Phi values\n",
    "        bundle_gradients = torch.stack([item[2] for item in bundle])  # Gradient values\n",
    "\n",
    "        # Construct the quadratic approximation model\n",
    "        diff = xi - bundle_points\n",
    "        model_phi = bundle_phis + torch.sum(bundle_gradients * diff, dim=1)\n",
    "        proximal_term = (zeta / 2) * norm(diff, dim=1)**2\n",
    "        subproblem_objective = model_phi + proximal_term\n",
    "\n",
    "        # Determine the next xi by minimizing the subproblem objective\n",
    "        best_idx = torch.argmax(subproblem_objective)\n",
    "        xi_next = bundle_points[best_idx] + (1 / zeta) * bundle_gradients[best_idx]\n",
    "\n",
    "        # Clip xi to enforce constraints\n",
    "        xi_next = torch.clamp(xi_next, min=0.01, max=upper_c)\n",
    "\n",
    "        # Check for convergence\n",
    "        if norm(xi_next - xi) < epsilon:\n",
    "            break\n",
    "\n",
    "        # Update xi for the next iteration\n",
    "        xi = xi_next.clone()\n",
    "        \n",
    "    return xi, lambda_plus, x_i_star, phi\n",
    "\n",
    "def initialize_weights(model, min_w, max_w):\n",
    "    \"\"\"\n",
    "    Initializes the weights of a given model using a uniform distribution.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The neural network model whose weights need initialization.\n",
    "        min_w (float): Minimum value for weight initialization.\n",
    "        max_w (float): Maximum value for weight initialization.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        torch.nn.init.uniform_(param, a=min_w, b=max_w)\n",
    "\n",
    "def train_and_evaluate(C, lr, lambda_reg, alpha, subgradient_step, w0, r, \n",
    "                       target_acc, target_entr, min_xi, max_xi, n_epochs, device, \n",
    "                       train_optimizer, entropy_optimizer, trainloader, testloader, delta):\n",
    "    \n",
    "    model = LeNet5().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if(train_optimizer == 'A'):\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=lambda_reg * alpha)\n",
    "    elif(train_optimizer == 'S'):\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=lambda_reg * alpha)\n",
    "    \n",
    "    # Parameters initialization\n",
    "    min_w, max_w = w0 - r, w0 + r\n",
    "    v = torch.linspace(min_w, max_w - (max_w - min_w)/C, steps=C)\n",
    "    initialize_weights(model, min_w, max_w)    \n",
    "    w = torch.cat([param.data.view(-1) for param in model.parameters()])\n",
    "    upper_c, lower_c = w.size(0), 1e-2\n",
    "    xi = min_xi + (max_xi - min_xi) * torch.rand(C, device=device)    \n",
    "    xi = torch.sort(xi)[0]   \n",
    "    entropy, accuracy = 0, 0\n",
    "    accuracies, entropies, distinct_weights = [], [], []\n",
    "    zeta, l = 50000, 0.5\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            w = torch.cat([param.data.view(-1) for param in model.parameters()])\n",
    "            #unique_weights = torch.unique(w).numel() \n",
    "            #indices = torch.searchsorted(v, w, right=True) - 1\n",
    "            #indices = torch.clamp(indices, min=0)\n",
    "            #w_quantized = v[indices]\n",
    "\n",
    "            zeta *= 1 + l\n",
    "            l = l / 1.5\n",
    "            if(entropy_optimizer == 'F'):\n",
    "                #xi, beta_tensor, x_star, phi = FISTA(xi, v, w_quantized, C, subgradient_step, max_iterations=15) \n",
    "                xi, beta_tensor, x_star, phi = FISTA(xi, v, w, C, subgradient_step, delta, max_iterations=15) \n",
    "            elif(entropy_optimizer == 'PM'):\n",
    "                #xi, beta_tensor, x_star, phi = ProximalBM(xi, v, w_quantized, C, zeta, subgradient_step, max_iterations=15) \n",
    "                xi, beta_tensor, x_star, phi = ProximalBM(xi, v, w, C, zeta, subgradient_step, delta, max_iterations=15)      \n",
    "            \n",
    "            # Update of ∇ɸ\n",
    "            idx = 0\n",
    "            for param in model.parameters():\n",
    "                numel = param.numel()\n",
    "                if param.grad is not None:\n",
    "                    param_grad = param.grad.view(-1)\n",
    "                else:\n",
    "                    param_grad = torch.zeros_like(param.data.view(-1))\n",
    "                param_grad += (1 - alpha) * lambda_reg * beta_tensor[idx:idx + numel]\n",
    "                param.grad = param_grad.view(param.size())\n",
    "                idx += numel\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        w = torch.cat([param.data.view(-1) for param in model.parameters()])\n",
    "        \n",
    "        entropy = round(compute_entropy(w.tolist())) + 1\n",
    "        entropies.append(entropy)\n",
    "        accuracy = test_accuracy(model, testloader, device)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"C={C}, lr={lr}, lambda_reg={lambda_reg}, \"\n",
    "              f\"alpha={alpha}, subgradient_step={subgradient_step}, w0={w0}, r={r}, \"\n",
    "              f\"target_acc={target_acc}, target_entr={target_entr}, \"\n",
    "              f\"min_xi={min_xi}, max_xi={max_xi}, n_epochs={n_epochs}, train_optimizer={train_optimizer} \"\n",
    "              f\"entropy_optimizer={entropy_optimizer}\")\n",
    "        print(\"\\nEpoch:\", epoch+1)\n",
    "        print(\"\\nAccuracies:\", accuracies)\n",
    "        print(\"\\nEntropies:\", entropies)\n",
    "        print(\"\\nMax Accuracy:\", max(accuracies))\n",
    "        print(\"Min entropy:\", min(entropies))\n",
    "\n",
    "        # Saving a better model\n",
    "        if(accuracy >= target_acc and entropy <= target_entr):\n",
    "            print(\"💥💥💥💥💥💥💥\\n💥ATTENTION!💥\\n💥💥💥💥💥💥💥\")\n",
    "            torch.save(model.state_dict(), f\"BestModelsBeforeQuantization/C{C}_r{round(r*1000)}.pth\")\n",
    "            target_acc = accuracy\n",
    "            target_entr = entropy\n",
    "        \n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        # Entropy exit conditions\n",
    "        if(epoch > 20 and entropy > 600000):\n",
    "            print(\"Entropy is not decreasing enough! (A)\")\n",
    "            return accuracy, entropy, target_acc, target_entr\n",
    "        if(epoch > 50):\n",
    "            if(entropies[-1] > 200000 and entropies[-2] > 200000 and entropies[-3] > 200000 and entropies[-4] > 200000):\n",
    "                print(\"Entropy is not decreasing enough! (B)\")\n",
    "                return accuracy, entropy, target_acc, target_entr           \n",
    "            \n",
    "        # Accuracy exit condition\n",
    "        if(epoch == 1 and accuracies[-1] < 70):\n",
    "            print(\"Accuracy is too low! (C)\")\n",
    "            return accuracy, entropy, target_acc, target_entr                    \n",
    "        if(epoch > 10):\n",
    "            if(accuracies[-1] < 90 and accuracies[-2] < 90 and accuracies[-3] < 90 and accuracies[-4] < 90):\n",
    "                print(\"Accuracy is too low! (D)\")\n",
    "                return accuracy, entropy, target_acc, target_entr     \n",
    "        \n",
    "        # ... ADD OTHER EXIT CONDITIONS ...      \n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Time taken for a epoch: {training_time:.2f} seconds\\n\")\n",
    "              \n",
    "    return accuracy, entropy, target_acc, target_entr\n",
    "\n",
    "# Select the computing device: use GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define a transformation: convert images to tensors\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "# Load the MNIST training dataset with the defined transformation\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# Create a DataLoader for the training set with batch size 64, shuffling enabled, and 4 worker threads\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\n",
    "# Load the MNIST test dataset with the same transformation\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "# Create a DataLoader for the test set with batch size 1000, shuffling disabled, and 4 worker threads\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False, num_workers=4)\n",
    "\n",
    "np.set_printoptions(precision=6)\n",
    "\n",
    "# Grid search \n",
    "param_grid = {\n",
    "    \"C\": [6, 256],  # Number of buckets of quantization\n",
    "    \"lr\": [0.0007], # Learning rate for the optimizer\n",
    "    \"lambda_reg\": [0.0015], # Regularization factor\n",
    "    \"alpha\": [0.533], # Percentage of standard regularization wrt entropic one \n",
    "    \"subgradient_step\": [1e5],  # Step size for subgradient\n",
    "    \"w0\": [-0.11], # Initial weight parameters\n",
    "    \"r\": [1.1],\n",
    "    \"target_acc\": [98.99], # Target accuracy percentage\n",
    "    \"target_entr\": [0.99602e6], # Target entropy threshold \n",
    "    \"min_xi\": [0], # lower bound for xi initialization\n",
    "    \"max_xi\": [1],  # upper bound for xi initialization\n",
    "    \"n_epochs\": [100], # Number of training epochs\n",
    "    \"device\": [device], # Computing device (GPU or CPU)\n",
    "    \"train_optimizer\": ['A'],  # 'A' for Adam, and 'S' for SGD\n",
    "    \"entropy_optimizer\": ['F'], # 'F' for FISTA, 'PM' for proximal bundle\n",
    "    \"trainloader\": [trainloader],  # Training data loader\n",
    "    \"testloader\": [testloader], # Test data loader\n",
    "    \"delta\": [0.1, 0.2, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "combination = 0\n",
    "\n",
    "for (C, lr, lambda_reg, alpha, subgradient_step, w0, r, \n",
    "     target_acc, target_entr, min_xi, max_xi, n_epochs, \n",
    "     device, train_optimizer, entropy_optimizer, trainloader, \n",
    "     testloader, delta) in product(param_grid[\"C\"],\n",
    "                            param_grid[\"lr\"],\n",
    "                            param_grid[\"lambda_reg\"],\n",
    "                            param_grid[\"alpha\"],\n",
    "                            param_grid[\"subgradient_step\"],\n",
    "                            param_grid[\"w0\"],\n",
    "                            param_grid[\"r\"],\n",
    "                            param_grid[\"target_acc\"],\n",
    "                            param_grid[\"target_entr\"],\n",
    "                            param_grid[\"min_xi\"],\n",
    "                            param_grid[\"max_xi\"],\n",
    "                            param_grid[\"n_epochs\"],\n",
    "                            param_grid[\"device\"],\n",
    "                            param_grid[\"train_optimizer\"],      \n",
    "                            param_grid[\"entropy_optimizer\"],   \n",
    "                            param_grid[\"trainloader\"], \n",
    "                            param_grid[\"testloader\"],\n",
    "                            param_grid[\"delta\"]\n",
    "                            ):\n",
    "    \n",
    "    # Counts combinations\n",
    "    combination += 1\n",
    "    \n",
    "    # Start training\n",
    "    start_time = time.time()\n",
    "    accuracy, entropy, target_acc, target_entr = train_and_evaluate(C=C,              \n",
    "                                                                lr=lr,           \n",
    "                                                                lambda_reg=lambda_reg,    \n",
    "                                                                alpha=alpha,          \n",
    "                                                                subgradient_step=subgradient_step, \n",
    "                                                                w0=w0,             \n",
    "                                                                r=r,              \n",
    "                                                                target_acc=target_acc,      \n",
    "                                                                target_entr=target_entr, \n",
    "                                                                min_xi=min_xi,              \n",
    "                                                                max_xi=max_xi,             \n",
    "                                                                n_epochs=n_epochs,        \n",
    "                                                                device=device,      \n",
    "                                                                train_optimizer=train_optimizer,     \n",
    "                                                                entropy_optimizer=entropy_optimizer,   \n",
    "                                                                trainloader=trainloader, \n",
    "                                                                testloader=testloader,\n",
    "                                                                delta=delta\n",
    "                                                            )\n",
    "        \n",
    "    training_time = time.time() - start_time\n",
    "    print(f'Time spent to train the model: {training_time:.2f} seconds\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6787a54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1, 1.102, 1.104, 1.106, 1.108, 1.11, 1.112, 1.114, 1.116, 1.118]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[round(1.1 + i * 0.002, 3) for i in range(10)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
