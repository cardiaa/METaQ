{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86735cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200: phi = -2.0198259353637695\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def knapsack_specialized(xi, v, w, C):\n",
    "    \"\"\"\n",
    "    Solves a specialized knapsack problem using a specialized method in a vectorized way\n",
    "\n",
    "    Args:\n",
    "        xi (torch.Tensor): xi variables.\n",
    "        v (torch.Tensor): Quantization vector.\n",
    "        w (torch.Tensor): Weight vector.\n",
    "        C (int): Number of buckets of quantization.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Optimal allocation (x_opt), optimal multipliers (lambda_opt), and objective values.\n",
    "    \"\"\"\n",
    "    \n",
    "    b_list = []\n",
    "    b = 0\n",
    "\n",
    "    # Compute breakpoint vector x_plus\n",
    "    while True:\n",
    "        delta_xi = (xi[b + 1:] - xi[b])\n",
    "        delta_v = (v[b + 1:] - v[b])\n",
    "        b = torch.argmin(delta_xi / delta_v) + 1 + b_list[-1] if b_list else 0\n",
    "\n",
    "        if b != C - 1:\n",
    "            b_list.append(int(b))\n",
    "\n",
    "        if b + 1 > C - 1:\n",
    "            break\n",
    "    b_list.append(C - 1)\n",
    "    x_plus = torch.zeros(C, dtype=torch.int32)\n",
    "    b_tensor = torch.tensor(b_list, dtype=torch.int32)\n",
    "    x_plus[b_tensor] = 1\n",
    "\n",
    "    # Determine optimal allocation based on w\n",
    "    w_idx = torch.searchsorted(v, w) \n",
    "    indices_breakpoints = torch.nonzero(x_plus == 1).squeeze()\n",
    "\n",
    "    # Creation of masks for extreme cases\n",
    "    mask_right = w > v[-1]\n",
    "    mask_left = w < v[0]\n",
    "\n",
    "    # Find indices using searchsorted\n",
    "    search_idx = torch.searchsorted(indices_breakpoints, w_idx)\n",
    "\n",
    "    # Ensure that the indices are valid\n",
    "    search_idx = torch.clamp(search_idx, 1, len(indices_breakpoints) - 1)\n",
    "\n",
    "    # Initialize idx_right and idx_left with the result of the search\n",
    "    idx_right = indices_breakpoints[search_idx]\n",
    "    idx_left = indices_breakpoints[search_idx - 1]\n",
    "\n",
    "    # Correct the indices for extreme cases\n",
    "    idx_right = torch.where(mask_right, indices_breakpoints[-1], idx_right)\n",
    "    idx_left = torch.where(mask_right, indices_breakpoints[-1], idx_left)\n",
    "\n",
    "    # Correct the indices for the case when w < v[0]\n",
    "    idx_right = torch.where(mask_left, indices_breakpoints[0], idx_right)\n",
    "    idx_left = torch.where(mask_left, indices_breakpoints[0], idx_left)\n",
    "\n",
    "    # Compute convex combination for optimal solution\n",
    "    x1, x2 = torch.zeros(2, len(w), C, dtype=torch.float32)\n",
    "\n",
    "    x1[torch.arange(len(w)), idx_left] = 1\n",
    "    x2[torch.arange(len(w)), idx_right] = 1\n",
    "\n",
    "    numerator = w - torch.matmul(x2, v)\n",
    "    denominator = torch.matmul((x1 - x2), v)\n",
    "    theta = numerator / denominator\n",
    "\n",
    "    mask_equal = (x1 == x2)\n",
    "    theta_expanded = theta.unsqueeze(1)\n",
    "    x_opt = torch.where(mask_equal, x1, x1 * theta_expanded + x2 * (1 - theta_expanded))\n",
    "\n",
    "    # Compute optimal multipliers\n",
    "    denominator = (v[idx_right] - v[idx_left])\n",
    "    denominator_zero_mask = denominator == 0\n",
    "\n",
    "    lambda_opt_nonzero = (xi[idx_right] - xi[idx_left]) / denominator\n",
    "    lambda_opt_zero_full = xi / v\n",
    "    lambda_opt_zero_full[0] = 0\n",
    "    lambda_opt_zero = lambda_opt_zero_full[idx_left]\n",
    "\n",
    "    lambda_opt = torch.where(denominator_zero_mask, lambda_opt_zero, lambda_opt_nonzero)\n",
    "\n",
    "    # Compute objective function values\n",
    "    objective_values = torch.matmul(x_opt, xi)\n",
    "\n",
    "    return x_opt, lambda_opt, objective_values\n",
    "\n",
    "# Initial parameters\n",
    "max_iterations = 200\n",
    "lambda_reg = 100  # Regularization parameter\n",
    "C = 5\n",
    "N = 3\n",
    "upper_c = N\n",
    "xi = torch.tensor([0.3451, 0.3661, 0.4231, 0.7080, 0.9156])\n",
    "v = torch.linspace(0, (C - 1)/C, C)\n",
    "w = torch.tensor([0.4, 0.6, 0])\n",
    "\n",
    "# Variables for FISTA\n",
    "xi_prev = xi.clone()\n",
    "t_prev = 1.0\n",
    "\n",
    "# Function to compute phi and the gradient\n",
    "def compute_phi_and_gradient(xi, v, w, C, upper_c):\n",
    "    \"\"\"\n",
    "    Compute the function phi and its gradient.\n",
    "\n",
    "    Parameters:\n",
    "    xi: Current solution vector.\n",
    "    v: Ordered vector of size C.\n",
    "    w: Real value belonging to v.\n",
    "    C: Problem dimension.\n",
    "    upper_c: Upper bound for constraint handling.\n",
    "\n",
    "    Returns:\n",
    "    phi: Computed phi value.\n",
    "    g: Gradient vector.\n",
    "    \"\"\"\n",
    "    x_i_star, lambda_plus, phi_plus = knapsack_specialized(xi, v, w, C)\n",
    "    sum_x_star = torch.sum(x_i_star, dim=0)\n",
    "    c_star = torch.exp(torch.log(torch.tensor(2.0)) * xi - 1)\n",
    "    c_star = torch.clamp(c_star, min=0.01, max=upper_c)\n",
    "    \n",
    "    # Compute phi terms\n",
    "    phi1 = torch.sum(c_star * torch.log(c_star) / torch.log(torch.tensor(2.0)))\n",
    "    phi2 = -torch.sum(xi * c_star)\n",
    "    phi3 = torch.sum(xi * sum_x_star)\n",
    "    phi = phi1 + phi2 + phi3\n",
    "    \n",
    "    # Compute the gradient (super-gradient)\n",
    "    g = -(c_star - sum_x_star)\n",
    "    return phi, g\n",
    "\n",
    "# Main FISTA loop\n",
    "for iteration in range(1, max_iterations + 1):\n",
    "    # Accelerated step (momentum)\n",
    "    t_current = (1 + torch.sqrt(torch.tensor(1) + 4 * t_prev**2)) / 2\n",
    "    y = xi + ((t_prev - 1) / t_current) * (xi - xi_prev)\n",
    "    \n",
    "    # Compute phi and gradient at y\n",
    "    phi, g = compute_phi_and_gradient(y, v, w, C, upper_c)\n",
    "    \n",
    "    # Proximal update step\n",
    "    xi_next = y + (1 / lambda_reg) * g  # Proximal update\n",
    "    \n",
    "    # Clipping to ensure constraints on xi\n",
    "    xi_next = torch.clamp(xi_next, min=0.01, max=upper_c)\n",
    "    \n",
    "    # Display progress\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Iteration {iteration}: phi = {phi.item()}\")\n",
    "    time.sleep(0.01)\n",
    "    \n",
    "    # Update for the next iteration\n",
    "    xi_prev = xi.clone()\n",
    "    xi = xi_next.clone()\n",
    "    t_prev = t_current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "130a4331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000: phi = -2.0054593086242676\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.linalg import norm\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Initial parameters\n",
    "max_iterations = 1000\n",
    "lambda_reg = 100  # Regularization parameter\n",
    "C = 5\n",
    "N = 3\n",
    "upper_c = N\n",
    "xi = torch.tensor([0.3451, 0.3661, 0.4231, 0.7080, 0.9156], requires_grad=False)\n",
    "v = torch.linspace(0, (C - 1)/C, C)\n",
    "w = torch.tensor([0.4, 0.6, 0], requires_grad=False)\n",
    "\n",
    "# Parameters for the Bundle method\n",
    "epsilon = 1e-5  # Convergence tolerance\n",
    "bundle_size = 5  # Maximum bundle size\n",
    "bundle = []  # Initial bundle (list of points, phi values, and gradients)\n",
    "\n",
    "# Main loop of the Bundle Proximal method\n",
    "for iteration in range(1, max_iterations + 1):\n",
    "    # Compute phi and gradient at the current point\n",
    "    phi, g = compute_phi_and_gradient(xi, v, w, C, upper_c)\n",
    "    \n",
    "    # Add the current point to the bundle\n",
    "    bundle.append((xi.clone(), phi, g.clone()))\n",
    "    if len(bundle) > bundle_size:\n",
    "        bundle.pop(0)  # Remove the oldest point if the bundle exceeds the maximum size\n",
    "    \n",
    "    # Solve the subproblem (regularized quadratic minimization)\n",
    "    bundle_points = torch.stack([item[0] for item in bundle])  # Bundle points\n",
    "    bundle_phis = torch.tensor([item[1] for item in bundle])  # Phi values\n",
    "    bundle_gradients = torch.stack([item[2] for item in bundle])  # Bundle gradients\n",
    "    \n",
    "    # Construct the regularized quadratic problem\n",
    "    diff = xi - bundle_points\n",
    "    model_phi = bundle_phis + torch.sum(bundle_gradients * diff, dim=1)\n",
    "    proximal_term = (lambda_reg / 2) * norm(diff, dim=1)**2\n",
    "    subproblem_objective = model_phi + proximal_term\n",
    "    \n",
    "    # Find the new point by minimizing the model\n",
    "    best_idx = torch.argmin(subproblem_objective)\n",
    "    xi_next = bundle_points[best_idx] + (1 / lambda_reg) * bundle_gradients[best_idx]\n",
    "    \n",
    "    # Clipping to ensure constraints on xi\n",
    "    xi_next = torch.clamp(xi_next, min=0.01, max=upper_c)\n",
    "    \n",
    "    # Convergence check\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(0.01)\n",
    "    print(f\"Iteration {iteration}: phi = {phi.item()}\")\n",
    "    \n",
    "    if norm(xi_next - xi) < epsilon:\n",
    "        print(\"Convergence reached!\")\n",
    "        break\n",
    "    \n",
    "    # Update xi\n",
    "    xi = xi_next.clone()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
