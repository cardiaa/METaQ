{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa6f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Define the LeNet-5 architecture\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # First convolutional layer: input 1 channel, output 6 channels, kernel size 5x5\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # Average pooling layer with kernel size 2x2 and stride 2\n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        # Second convolutional layer: input 6 channels, output 16 channels, kernel size 5x5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)  # Flattened input size 16*4*4, output 120\n",
    "        self.fc2 = nn.Linear(120, 84)  # Output 84 neurons\n",
    "        self.fc3 = nn.Linear(84, 10)  # Output 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply first convolution, then activation function (tanh), then pooling\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # Apply second convolution, then activation function (tanh), then pooling\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        # Pass through fully connected layers with activation (tanh)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        # Output layer without activation (raw scores for classification)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "def model_to_json(model):\n",
    "    \"\"\"\n",
    "    Function to convert a PyTorch model to a JSON representation\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if len(list(module.children())) == 0:  # Consider only terminal modules (not containers)\n",
    "            layer = {\"name\": name, \"type\": module.__class__.__name__}\n",
    "            \n",
    "            # Add specific parameters based on the layer type\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                layer.update({\n",
    "                    \"in_channels\": module.in_channels,\n",
    "                    \"out_channels\": module.out_channels,\n",
    "                    \"kernel_size\": module.kernel_size,\n",
    "                    \"stride\": module.stride,\n",
    "                    \"padding\": module.padding\n",
    "                })\n",
    "            elif isinstance(module, nn.AvgPool2d):\n",
    "                layer.update({\n",
    "                    \"kernel_size\": module.kernel_size,\n",
    "                    \"stride\": module.stride\n",
    "                })\n",
    "            elif isinstance(module, nn.Linear):\n",
    "                layer.update({\n",
    "                    \"in_features\": module.in_features,\n",
    "                    \"out_features\": module.out_features\n",
    "                })\n",
    "            layers.append(layer)\n",
    "    return {\"layers\": layers}\n",
    "\n",
    "def json_to_model(json_file):\n",
    "    \"\"\"\n",
    "    Function to load a model from a JSON file\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\") as f:\n",
    "        model_data = json.load(f)\n",
    "    \n",
    "    # Manually create a LeNet-5 model instance\n",
    "    model = LeNet5()  # Directly instantiate the LeNet-5 class\n",
    "    layers = model.children()  # Retrieve model layers\n",
    "    \n",
    "    for layer_data, layer in zip(model_data[\"layers\"], layers):\n",
    "        layer_type = layer_data[\"type\"]\n",
    "        \n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            # Randomly initialize weights and biases for Conv2d layers\n",
    "            layer.weight.data = torch.randn_like(layer.weight.data)\n",
    "            layer.bias.data = torch.randn_like(layer.bias.data)\n",
    "        \n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            # Randomly initialize weights and biases for Linear layers\n",
    "            layer.weight.data = torch.randn_like(layer.weight.data)\n",
    "            layer.bias.data = torch.randn_like(layer.bias.data)\n",
    "        \n",
    "        # Additional logic can be added to update other layer types if necessary\n",
    "        \n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "# Creates the model\n",
    "model = LeNet5()\n",
    "\n",
    "# Conversion in JSON\n",
    "model_dict = model_to_json(model)\n",
    "json_file = \"lenet5.json\"\n",
    "with open(json_file, \"w\") as f:\n",
    "    json.dump(model_dict, f, indent=4)\n",
    "\n",
    "print(f\"FILE {json_file} CREATED\")\n",
    "\"\"\"\n",
    "\n",
    "def test_accuracy(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Function to calculate the accuracy of a model on a given dataloader.\n",
    "    \"\"\"\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to the appropriate device\n",
    "            outputs = model(images)  # Get model predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the class with the highest probability\n",
    "            total += labels.size(0)  # Update total number of samples\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "    \n",
    "    accuracy = 100 * correct / total  # Compute accuracy percentage\n",
    "    return accuracy\n",
    "\n",
    "def compute_entropy(string):\n",
    "    \"\"\"\n",
    "    Function to compute the entropy of a given string.\n",
    "    \"\"\"\n",
    "    # Count the frequency of each character in the string\n",
    "    frequencies = Counter(string)\n",
    "    # Calculate the total length of the string\n",
    "    total_length = len(string)\n",
    "    \n",
    "    # Compute entropy\n",
    "    entropy = 0\n",
    "    for freq in frequencies.values():\n",
    "        probability = freq / total_length  # Compute probability of each character\n",
    "        entropy -= probability * math.log2(probability)  # Apply entropy formula\n",
    "    \n",
    "    return entropy * total_length  # Return the entropy weighted by string length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ece460",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON file: lenet5.json\n",
      "File size: 10656 bits\n"
     ]
    }
   ],
   "source": [
    "# Initialize the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the MNIST test dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Load the network from JSON and calculate its size\n",
    "json_file = \"lenet5.json\"\n",
    "model = json_to_model(json_file).to(device)\n",
    "\n",
    "# Compute the file size in bits\n",
    "architecture_size_bytes = os.path.getsize(json_file)\n",
    "architecture_size_bits = architecture_size_bytes * 8\n",
    "\n",
    "# Output information\n",
    "print(f\"Saved JSON file: {json_file}\")\n",
    "print(f\"File size: {architecture_size_bits} bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0370ca11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique weights in the original model: 6751\n",
      "Original model accuracy: 98.95%\n",
      "\n",
      "Entropy of the string multiplied by its length: 114234\n"
     ]
    }
   ],
   "source": [
    "# Load model weights\n",
    "model_name = \"r1114_1\"\n",
    "model_path = \"../BestModelsBeforeQuantization/\" + model_name + \".pth\"\n",
    "#model_path = \"../BestModelsAfterQuantization/\" + model_name + \"_quantized.pth\"\n",
    "\n",
    "w0 = -0.11\n",
    "r = 1.114\n",
    "min_w = w0 - r\n",
    "max_w = w0 + r\n",
    "\n",
    "state_dict = torch.load(model_path, map_location=device, weights_only=False)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "w_saved = torch.cat([param.data.view(-1) for param in model.parameters()])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "num_unique_weights_saved = torch.unique(w_saved).numel()\n",
    "original_accuracy = test_accuracy(model, testloader, device)\n",
    "print(f\"Number of unique weights in the original model: {num_unique_weights_saved}\")\n",
    "print(f\"Original model accuracy: {original_accuracy:.2f}%\")\n",
    "\n",
    "encoded_list = []\n",
    "for elem in w_saved:\n",
    "    loaded_element = float(elem)\n",
    "    if loaded_element == -0.0:\n",
    "        loaded_element = 0.0\n",
    "    encoded_list.append(loaded_element)\n",
    "\n",
    "entropy = round(compute_entropy(encoded_list)) + 1\n",
    "print(f\"\\nEntropy of the string multiplied by its length: {entropy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e87333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization at C=140, Accuracy:99.01, Entropy:59601\n",
      "Quantization at C=386, Accuracy:99.01, Entropy:70135\n",
      "Quantization at C=331, Accuracy:99.01, Entropy:68712\n",
      "Quantization at C=327, Accuracy:99.0, Entropy:68583\n",
      "Quantization at C=322, Accuracy:99.0, Entropy:68531\n",
      "Quantization at C=678, Accuracy:98.99, Entropy:76202\n",
      "Quantization at C=220, Accuracy:98.99, Entropy:63668\n",
      "Quantization at C=350, Accuracy:98.99, Entropy:69694\n",
      "Quantization at C=377, Accuracy:98.99, Entropy:70439\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def quantize_weights_center(weights, v, v_centers):\n",
    "    \"\"\"\n",
    "    Function for weight quantization using central value.\n",
    "    Quantizes weights based on the central value of the buckets in vector v.\n",
    "    \"\"\"\n",
    "    indices = torch.bucketize(weights, v, right=False) - 1\n",
    "    indices = torch.clamp(indices, min=0, max=len(v_centers) - 1)  # Ensure indices are valid\n",
    "    return v_centers[indices]\n",
    "\n",
    "l1 = []\n",
    "l2 = []\n",
    "\n",
    "for C in range(1, 1024):\n",
    "    # Construct vector v\n",
    "    v = torch.linspace(min_w, max_w - (max_w - min_w)/C, steps=C)\n",
    "\n",
    "    # Compute central values of the buckets\n",
    "    v_centers = (v[:-1] + v[1:]) / 2\n",
    "    v_centers = torch.cat([v_centers, v[-1:]])  # Add final value to handle the last bucket\n",
    "\n",
    "    state_dict = torch.load(model_path, map_location='cpu', weights_only=False)  # Adjust device accordingly\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Extract model weights\n",
    "    w_saved = torch.cat([param.data.view(-1) for param in model.parameters()])\n",
    "\n",
    "    # Quantize weights using central values\n",
    "    w_quantized = quantize_weights_center(w_saved, v, v_centers)\n",
    "\n",
    "    # Replace quantized weights in the model\n",
    "    start_idx = 0\n",
    "    for param in model.parameters():\n",
    "        numel = param.data.numel()\n",
    "        param.data = w_quantized[start_idx:start_idx + numel].view(param.data.size())\n",
    "        start_idx += numel\n",
    "\n",
    "    # Evaluate quantized model\n",
    "    model.eval()\n",
    "    num_unique_weights_quantized = torch.unique(w_quantized).numel()\n",
    "    quantized_accuracy = test_accuracy(model, testloader, device)\n",
    "\n",
    "    # Compute entropy of the quantized string\n",
    "    encoded_list = [float(elem) if float(elem) != -0.0 else 0.0 for elem in w_quantized]\n",
    "    entropy = round(compute_entropy(encoded_list)) + 1\n",
    "    \n",
    "    l1.append(quantized_accuracy)\n",
    "    l2.append(entropy)\n",
    "    \n",
    "    print(f\"C = {C} analyzed\")\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "sorted_indices = np.argsort(l1)\n",
    "for i in range(1, 10):\n",
    "    print(f\"Quantization at C={sorted_indices[-i] + 1}, Accuracy:{l1[sorted_indices[-i]]}, Entropy:{l2[sorted_indices[-i]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82811348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique weights in the quantized model: 127\n",
      "Quantized model accuracy: 99.01%\n",
      "\n",
      "Entropy of the string multiplied by its length: 59601\n",
      "\n",
      "Model quantized saved!\n"
     ]
    }
   ],
   "source": [
    "# Quantization with C = 140\n",
    "C = 140\n",
    "\n",
    "# Construct vector v\n",
    "v = torch.linspace(min_w, max_w - (max_w - min_w)/C, steps=C)\n",
    "\n",
    "# Compute central values of the buckets\n",
    "v_centers = (v[:-1] + v[1:]) / 2\n",
    "v_centers = torch.cat([v_centers, v[-1:]])  # Add final value to handle the last bucket\n",
    "\n",
    "state_dict = torch.load(model_path, map_location='cpu', weights_only=False)  # Adjust device accordingly\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Extract model weights\n",
    "w_saved = torch.cat([param.data.view(-1) for param in model.parameters()])\n",
    "\n",
    "# Quantize weights using central values\n",
    "w_quantized = quantize_weights_center(w_saved, v, v_centers)\n",
    "\n",
    "# Replace quantized weights in the model\n",
    "start_idx = 0\n",
    "for param in model.parameters():\n",
    "    numel = param.data.numel()\n",
    "    param.data = w_quantized[start_idx:start_idx + numel].view(param.data.size())\n",
    "    start_idx += numel\n",
    "\n",
    "# Evaluate quantized model\n",
    "model.eval()\n",
    "num_unique_weights_quantized = torch.unique(w_quantized).numel()\n",
    "quantized_accuracy = test_accuracy(model, testloader, device)\n",
    "\n",
    "print(f\"Number of unique weights in the quantized model: {num_unique_weights_quantized}\")\n",
    "print(f\"Quantized model accuracy: {quantized_accuracy:.2f}%\")\n",
    "\n",
    "# Compute entropy of the quantized string\n",
    "encoded_list = [float(elem) if float(elem) != -0.0 else 0.0 for elem in w_quantized]\n",
    "entropy = round(compute_entropy(encoded_list)) + 1\n",
    "print(f\"\\nEntropy of the string multiplied by its length: {entropy}\")\n",
    "\n",
    "model_path = \"../BestModelsAfterQuantization/\" + model_name + \"_quantized.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"\\nModel quantized saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77cbd4c",
   "metadata": {},
   "source": [
    "## Compression: Huffman Coding ( See https://rosettacode.org/wiki/Huffman_coding#Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a299ddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding successful!\n",
      "Compressed size in bits: 77872\n",
      "Uncompressed size in bits: 1421632\n",
      "\n",
      "Compression ratio: 5.48 %\n"
     ]
    }
   ],
   "source": [
    "from heapq import heappush, heappop, heapify\n",
    "from collections import defaultdict\n",
    "\n",
    "def encode(symb2freq):\n",
    "    \"\"\"Huffman encode the given dict mapping symbols to weights\"\"\"\n",
    "    heap = [[wt, [sym, \"\"]] for sym, wt in symb2freq.items()]\n",
    "    heapify(heap)\n",
    "    while len(heap) > 1:\n",
    "        lo = heappop(heap)\n",
    "        hi = heappop(heap)\n",
    "        for pair in lo[1:]:\n",
    "            pair[1] = '0' + pair[1]\n",
    "        for pair in hi[1:]:\n",
    "            pair[1] = '1' + pair[1]\n",
    "        heappush(heap, [lo[0] + hi[0]] + lo[1:] + hi[1:])\n",
    "    return sorted(heappop(heap)[1:], key=lambda p: (len(p[-1]), p))\n",
    "\n",
    "def decode(encoded_text, huffman_code):\n",
    "    \"\"\"\n",
    "    Decodes the given encoded text using the provided Huffman code.\n",
    "    \"\"\"\n",
    "    code_to_symbol = {code: symbol for symbol, code in huffman_code}\n",
    "    decoded_text = []\n",
    "    temp_code = \"\"\n",
    "    for bit in encoded_text:\n",
    "        temp_code += bit\n",
    "        if temp_code in code_to_symbol:\n",
    "            decoded_text.append(code_to_symbol[temp_code])\n",
    "            temp_code = \"\"\n",
    "    return decoded_text\n",
    "\n",
    "# Apply Huffman encoding\n",
    "symb2freq = defaultdict(int)\n",
    "for sym in encoded_list:\n",
    "    symb2freq[sym] += 1\n",
    "huff = encode(symb2freq)\n",
    "encoded_huffman = ''.join([dict(huff)[sym] for sym in encoded_list])\n",
    "decoded_list = decode(encoded_huffman, huff)\n",
    "\n",
    "if encoded_list == decoded_list:\n",
    "    print(\"\\nEncoding successful!\")\n",
    "else:\n",
    "    print(\"Error during encoding!\")\n",
    "\n",
    "print(\"Compressed size in bits:\", len(encoded_huffman))\n",
    "print(\"Uncompressed size in bits:\", len(encoded_list) * 32)\n",
    "print(\"\\nCompression ratio:\", 100 * round(len(encoded_huffman) / (len(encoded_list) * 32), 4), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d246184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dimension: 1421632 bit\n",
      "\n",
      "Encoded list dimension: 44426 bit\n",
      "Huffman dictionary dimension: 5421 bit\n",
      "Network architecture dimension: 10656 bit\n",
      "\n",
      "Total dimension: 60503 bit\n",
      "\n",
      "Compression ratio: 4.26%\n"
     ]
    }
   ],
   "source": [
    "dictionary_size = 0\n",
    "for sym, code in huff:\n",
    "    dictionary_size += 32 + len(code)\n",
    "\n",
    "# Total dimension\n",
    "total_size = len(encoded_list) + dictionary_size + architecture_size_bits\n",
    "\n",
    "# Compression ratio\n",
    "original_size = len(encoded_list) * 32  # Ogni peso è un float a 32 bit\n",
    "compression_ratio = total_size / original_size\n",
    "\n",
    "print(f\"Original dimension: {original_size} bit\")\n",
    "\n",
    "print(f\"\\nEncoded list dimension: {len(encoded_list)} bit\")\n",
    "print(f\"Huffman dictionary dimension: {dictionary_size} bit\")\n",
    "print(f\"Network architecture dimension: {architecture_size_bits} bit\")\n",
    "print(f\"\\nTotal dimension: {total_size} bit\")\n",
    "print(f\"\\nCompression ratio: {compression_ratio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb0a8d1",
   "metadata": {},
   "source": [
    "## Arithmetic coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86fde15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...STARTING ARITHMETIC CODING...\n",
      "Encoding time: 599.79 seconds\n",
      "...STARTING DECODING...\n",
      "Decoding time: 734.95 seconds\n",
      "\n",
      "ENCODING SUCCESSFUL!\n",
      "\n",
      "Arithmetic Coding achieves 59601 bits.\n",
      "Compression ratio: 4.1924%\n"
     ]
    }
   ],
   "source": [
    "## from collections import defaultdict\n",
    "from decimal import Decimal, getcontext\n",
    "import time\n",
    "\n",
    "def encode_arithmetic(symb2freq):\n",
    "    \"\"\"\n",
    "    Performs arithmetic encoding for a dictionary of symbols with their respective frequencies.\n",
    "    \"\"\"\n",
    "    total_freq = sum(symb2freq.values())  # Compute total frequency\n",
    "    prob_intervals = {}  # Dictionary to store probability intervals\n",
    "    low = Decimal(0.0)\n",
    "    \n",
    "    for sym, freq in sorted(symb2freq.items()):  # Iterate through symbols in sorted order\n",
    "        prob_intervals[sym] = (low, low + Decimal(freq) / Decimal(total_freq))  # Assign probability range\n",
    "        low = prob_intervals[sym][1]  # Update lower bound for next symbol\n",
    "    \n",
    "    return prob_intervals  # Return dictionary of intervals\n",
    "\n",
    "def encode_arithmetic_text(symbol_list, prob_intervals):\n",
    "    \"\"\"\n",
    "    Encodes a list of symbols using arithmetic encoding.\n",
    "    \"\"\"\n",
    "    low = Decimal(0.0)\n",
    "    high = Decimal(1.0)\n",
    "    \n",
    "    for sym in symbol_list:  # Process each symbol in the list\n",
    "        range_ = high - low  # Compute current range\n",
    "        high = low + range_ * prob_intervals[sym][1]  # Update upper bound\n",
    "        low = low + range_ * prob_intervals[sym][0]  # Update lower bound\n",
    "    \n",
    "    R = high - low  # Final range after encoding\n",
    "    dimension = - R.ln() / Decimal(2).ln()  # Compute encoding length in bits\n",
    "    return (low + high) / 2, dimension  # Return the midpoint representation and bit size\n",
    "\n",
    "def decode_arithmetic(encoded_value, symb2freq, length_of_symbols):\n",
    "    \"\"\"\n",
    "    Decodes an arithmetic-encoded value given the symbol frequency dictionary.\n",
    "    \"\"\"\n",
    "    prob_intervals = encode_arithmetic(symb2freq)  # Recompute probability intervals\n",
    "    decoded_symbols = []\n",
    "    \n",
    "    for _ in range(length_of_symbols):  # Decode for the original length of symbols\n",
    "        for sym, (low, high) in prob_intervals.items():  # Iterate through interval mappings\n",
    "            if low <= encoded_value < high:  # Check if value falls in symbol range\n",
    "                decoded_symbols.append(sym)  # Append decoded symbol\n",
    "                range_ = high - low  # Compute new range\n",
    "                encoded_value = (encoded_value - low) / range_  # Normalize value\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(f\"Unable to decode symbol. encoded_value: {encoded_value}\")\n",
    "    \n",
    "    return decoded_symbols  # Return decoded symbol list\n",
    "\n",
    "# Set higher precision for decimal operations\n",
    "getcontext().prec = round(len(encoded_list) * 1.4)\n",
    "\n",
    "# Create the frequency dictionary\n",
    "symb2freq = defaultdict(int)\n",
    "for sym in encoded_list:\n",
    "    symb2freq[sym] += 1  # Count symbol occurrences\n",
    "\n",
    "# Perform arithmetic encoding\n",
    "start_time = time.time()\n",
    "print(\"...STARTING ARITHMETIC CODING...\")\n",
    "prob_intervals = encode_arithmetic(symb2freq)  # Compute probability intervals\n",
    "encoded_value, dimension = encode_arithmetic_text(encoded_list, prob_intervals)  # Encode data\n",
    "final_time = time.time() - start_time\n",
    "print(f'Encoding time: {final_time:.2f} seconds')\n",
    "\n",
    "# Decode the encoded text\n",
    "print(\"...STARTING DECODING...\")\n",
    "start_time = time.time()\n",
    "decoded_symbols = decode_arithmetic(encoded_value, symb2freq, len(encoded_list))  # Decode data\n",
    "final_time = time.time() - start_time\n",
    "print(f'Decoding time: {final_time:.2f} seconds')\n",
    "\n",
    "# Verify that encoding and decoding are correct\n",
    "if encoded_list == decoded_symbols:\n",
    "    print(\"\\nENCODING SUCCESSFUL!\")\n",
    "else:\n",
    "    print(f\"Encoding error! Decoded: {decoded_symbols}\")\n",
    "\n",
    "print(\"\\nArithmetic Coding achieves\", round(dimension) + 1, \"bits.\")\n",
    "\n",
    "# Compute original file size\n",
    "bits_per_float = 32  # Assuming 32-bit float representation\n",
    "dimensione_originale = len(encoded_list) * bits_per_float  # Total bit size\n",
    "\n",
    "# Compute compression ratio\n",
    "compression_ratio = (round(dimension) + 1) / dimensione_originale\n",
    "\n",
    "# Output compression ratio\n",
    "print(f\"Compression ratio: {compression_ratio:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edf9868a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARITHMETIC CODING REACHES n*H? True\n"
     ]
    }
   ],
   "source": [
    "print(\"ARITHMETIC CODING REACHES n*H?\", entropy == round(dimension) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5964fe",
   "metadata": {},
   "source": [
    "## gzip-9 & zstd-22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "624a06a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENCODING SUCCESSFUL!\n",
      "\n",
      "ENCODING SUCCESSFUL!\n",
      "\n",
      "\n",
      "Original dimension: 1421632 bits\n",
      "Gzip-9 compressed dimension: 61336 bits (Compression Ratio: 4.31%)\n",
      "Zstd-22 compressed dimension: 48824 bits (Compression Ratio: 3.43%)\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import zstandard as zstd\n",
    "import struct\n",
    "\n",
    "# Converts float list in byte\n",
    "input_bytes = b''.join(struct.pack('f', num) for num in encoded_list)\n",
    "\n",
    "# Compresses with gzip-9\n",
    "def compress_gzip(data):\n",
    "    return gzip.compress(data, compresslevel=9)\n",
    "\n",
    "# Compresses with zstd-22\n",
    "def compress_zstd(data):\n",
    "    cctx = zstd.ZstdCompressor(level=22)  # Creates a compressor object with level 22\n",
    "    return cctx.compress(data)\n",
    "\n",
    "# Decompresses with gzip-9\n",
    "def decompress_gzip(data):\n",
    "    decompressed = gzip.decompress(data)\n",
    "    return list(struct.unpack(f'{len(decompressed) // 4}f', decompressed))  # Casts to float\n",
    "\n",
    "# Decompresses with zstd-22\n",
    "def decompress_zstd(data):\n",
    "    dctx = zstd.ZstdDecompressor()  # Creates a compressor object\n",
    "    decompressed = dctx.decompress(data)\n",
    "    return list(struct.unpack(f'{len(decompressed) // 4}f', decompressed))  # Casts to float\n",
    "\n",
    "def compare_lists(list1, list2, tollerance=1e-4):\n",
    "    if len(list1) != len(list2):\n",
    "        return False\n",
    "    return all(abs(a - b) <= tollerance for a, b in zip(list1, list2))\n",
    "\n",
    "# Compression\n",
    "gzip_compressed = compress_gzip(input_bytes)\n",
    "zstd_compressed = compress_zstd(input_bytes)\n",
    "\n",
    "# Decompression\n",
    "gzip_decompressed = decompress_gzip(gzip_compressed)\n",
    "zstd_decompressed = decompress_zstd(zstd_compressed)\n",
    "\n",
    "# Verifies\n",
    "if compare_lists(encoded_list, gzip_decompressed):\n",
    "    print(\"\\nENCODING SUCCESSFUL!\")\n",
    "else:\n",
    "    print(f\"Encoding error! Decoded\")\n",
    "    \n",
    "if compare_lists(encoded_list, zstd_decompressed):\n",
    "    print(\"\\nENCODING SUCCESSFUL!\")\n",
    "else:\n",
    "    print(f\"Encoding error! Decoded\")\n",
    "    \n",
    "# Calculates dimensions\n",
    "original_size_bits = len(input_bytes) * 8\n",
    "gzip_size = len(gzip_compressed) * 8\n",
    "zstd_size = len(zstd_compressed) * 8\n",
    "\n",
    "# Compression ratio\n",
    "gzip_ratio = gzip_size / original_size_bits\n",
    "zstd_ratio = zstd_size / original_size_bits\n",
    "\n",
    "# Output delle dimensioni e del rapporto di compressione\n",
    "print(f\"\\n\\nOriginal dimension: {original_size_bits} bits\")\n",
    "print(f\"Gzip-9 compressed dimension: {gzip_size} bits (Compression Ratio: {gzip_ratio:.2%})\")\n",
    "print(f\"Zstd-22 compressed dimension: {zstd_size} bits (Compression Ratio: {zstd_ratio:.2%})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
